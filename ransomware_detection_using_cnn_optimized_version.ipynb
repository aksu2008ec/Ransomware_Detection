{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Conv1D, MaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical, normalize\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "LABELS = {}\n",
    "counter = iter(range(20))\n",
    "\n",
    "def pad_and_convert(s):\n",
    "    \"\"\"Collect 2000 bytes from packet payload. If payload length is less than\n",
    "    2000 bytes, pad zeroes at the end. Then convert to integers and normalize.\"\"\"\n",
    "    if len(s) < 2000:\n",
    "        s += '00' * (2000-len(s))\n",
    "    else:\n",
    "        s = s[:2000]\n",
    "    return [float(int(s[i]+s[i+1], 16)/255) for i in range(0, 2000, 2)]\n",
    "\n",
    "def read_file(f, label):\n",
    "    df = pd.read_csv(f, index_col=None, header=0)\n",
    "    df.columns = ['data']\n",
    "    df['label'] = label\n",
    "    return df\n",
    "\n",
    "def preprocess(path):\n",
    "    files = glob.glob(os.path.join(path, '*.txt'))\n",
    "    list_ = []\n",
    "    for f in files:\n",
    "        label = f.split('/')[-1].split('.')[0]\n",
    "        LABELS[label] = next(counter)\n",
    "        labelled_df = partial(read_file, label=LABELS[label])\n",
    "        list_.append(labelled_df(f))\n",
    "    df = pd.concat(list_,ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def build_model(dropout_rate=0.5, optimizer='adam'):\n",
    "    activation = 'relu'\n",
    "    num_classes = len(LABELS)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(512, strides=2, input_shape=(1000, 1), activation=activation, kernel_size=3, padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Conv1D(256, strides=2, activation=activation, kernel_size=3, padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid')) #Using Sigmoid activation for binary classification\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount(r\"/content/gdrive\")\n",
    "    df = preprocess(path='/content/gdrive/My Drive/Dataset/')\n",
    "\n",
    "    df['data'] = df['data'].apply(pad_and_convert)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['data'], df['label'],\n",
    "                                                        test_size=0.3, random_state=4)\n",
    "\n",
    "    X_train = X_train.apply(pd.Series)\n",
    "    X_test = X_test.apply(pd.Series)\n",
    "    X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    classifier = KerasClassifier(build_fn=build_model)\n",
    "\n",
    "    parameters = {'batch_size': [32, 64, 128, 256, 512],\n",
    "                  'epochs': [300],\n",
    "                  'optimizer': ['adam','rmsdrop', 'adadelta'],\n",
    "                  'dropout_rate': [0.1, 0.3, 0.5, 0.8]}\n",
    "    print('calling GridSearchCV\\n')\n",
    "    grid_search = GridSearchCV(estimator=classifier,\n",
    "                               param_grid=parameters,\n",
    "                               scoring='accuracy',\n",
    "                               cv=3, verbose=10)\n",
    "\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    best_parameters = grid_result.best_params_\n",
    "    best_accuracy = grid_result.best_score_\n",
    "\n",
    "    print('Best parameters are: {}\\nBest score is: {}\\n'.format(best_parameters, best_accuracy))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"printing .... %f (%f) with: %r\\n\" % (mean, stdev, param))\n",
    "    \n",
    "    print(grid_result.best_params_)\n",
    "    grid_predictions  = grid_result.predict(X_test)\n",
    "    print('classificationÂ report\\n')\n",
    "    print(classification_report(y_test,grid_predictions))\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
